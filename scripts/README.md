**Note** : We do not provide an overall launcher for this project as the entire pipeline is computationally intensive and not feasible to run in a single session, especially not on a commercial computer. To enhance reproducibility, we provide detailed instructions on the order of operations and the expected outputs of the various scripts, along with the commands to execute them.

# 1. Pre-processing

Before executing other computations, all scripts within the pre_processing folder must be run. These scripts are independent of each other, so the order of execution within this subset does not matter. Primarily, these scripts generate files that are already available (precomputed) in the `data` folder; thus, this section is mainly for reproducibility purposes and can be optionally skipped. The most crucial scripts in this folder include:

- **process_reactome.py**: This script processes the raw structure of the Reactome database to produce mappings from pathways to genes, genes to pathways, and pathways to drugs, utilizing only Reactome data. Execute the script using the following command:

```bash
python process_reactome.py
```

- **get_pubchem_ids.py**: This script automates the mapping of free-text drug names to PubChem IDs. It requires the path to the data folder and the specific dataset for which the PubChem IDs should be retrieved (either `gdsc`, `prism`, or `all`). To run the script, use:

```bash
# Default (computes for all datasets and uses ./../../data as default path for the data folder)
python get_pubchem_ids.py

# Only gdsc
python get_pubchem_ids.py --dataset gdsc --data_path ./../../data 

# Only prism
python get_pubchem_ids.py --dataset prism --data_path ./../../data 
```

- **celligner_script.py**: This script aligns CCLE with TCGA transcriptomics data, and possibly additional external data, using Celligner. If external datasets need to be aligned (in our case, GBM and PDAC), they should be specified; otherwise, run the script with the following command:

```bash
python celligner_script.py
```

# 2. LLMs

- **2.1 fetch_abstracts.py**: This script is designed to fetch abstracts related to drug names from scientific databases and save them as JSON files. It requires specifying the path to the data folder, the dataset (either gdsc or prism), and an email address for API access. Additionally, the number of abstracts (k) to retrieve can be specified. Here's how to use the script:

```bash
#Default usage (uses default path and does not specify dataset or email):
python fetch_abstracts.py

#Specifying dataset and path:
python fetch_abstracts.py --dataset gdsc --data_path ./../../data

#Complete specification (includes email and number of abstracts):
python fetch_abstracts.py --dataset prism --data_path ./../../data --mail user@example.com --k 10
```

- **2.2 generate_drug_summaries.py**: This script is designed to generate textaul drug summaries starting from drug metadata and by using a LLM to process input drug data. It specifically configures the language model to run on a specified GPU, handles various paths for data and results, and accepts multiple command-line arguments to customize the process. Running with predefined parameters is recommended

```bash
#Default usage:
python generate_drug_summaries.py
```

- **2.3 refine_descriptions.py**: Starting from the descriptions generated by the previous code (2.2), this script is designed to generate refined descriptions integrating insights from abstracted fetched in 2.1 . Running with predefined parameters is recommended

```bash
#Default usage:
python refine_descriptions.py
```

- **2.4 pathway_selector.py**: This scripts uses a constrained LLM to select, starting from the generated textual description, which are the Reactome pathways most likely to be involved in determining the efficacy of a drug. The scripts allows for varying different parameters such as the number `pathway_number` to select, the number of self-consistency checks `self_k` to perform , the dataset on which the selection should be performed and whether computations should be done for a single drug or for automatically for multiple drugs (possibly coordinating different asynchronous processes). Here's how to use the script:

```bash
#Basic usage - running the procedure on a single drug on prism
python pathway_selector.py --drugID <your_drug_id, ex: 1909> --selection_mode single_drug --dataset prism #prism default dataset

#Basic usage - running the procedure on a single drug on gdsc
python pathway_selector.py --drugID <your_drug_id, ex: 1909> --selection_mode single_drug --dataset gdsc

#Change the number of self-consistency checks
python pathway_selector.py --drugID <your_drug_id, ex: 1909> --selection_mode single_drug --self_k 3 #default is 5

#Run for all the drugs in the dataset (first process that builds the coordination SQAlchemy object)
python pathway_selector.py --build_select_db --dataset prism  --selection_mode asynch

#Run for all the drugs in the dataset (automatically leverages the SQL object created by the command above)
python pathway_selector.py --dataset prism  --selection_mode asynch
```

- **2.5 drugs_pathway_annotation.py**: Combines all the info available on a drug to determine the set of genes to use for trainig the MOA-driven model. An important parameter of this script is `self_consistency_threshold` which represents the number of time a pathway needs to be selected in 2.4 in order to be retained (controlling hallucinations). Running with predefined parameters is recommended

```bash
#Default usage:
python drug_pathways_annotation.py

#Changing self consistency threshold
python drug_pathways_annotation.py --self_consistency_threshold 3
```

For completness we also report scripts `GPT4_description_prompt.txt` and `GPT4_pathway_prompt.txt` to perform the same procedure leveraging openAI propritary API.